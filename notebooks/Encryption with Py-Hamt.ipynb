{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a64bec-93ac-401d-b634-fd8437b3d380",
   "metadata": {},
   "source": [
    "# Creating an Encryption ETL - CPC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8979b10-4ad8-4e94-80ec-4e32d34785ff",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to the [dClimate](https://www.dclimate.net/) Encryption with Py-hamt - CPC Jupyter notebook. This notebook describes how to create an [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) to ingest GIS data into IPFS from a remote source and encrypt it. If you have not read the [Getting Started](./Getting%20Started.\n",
    "    ipynb) notebook we highly suggest you first go through that notebook in order to understand everything below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0119f-d6fb-463d-af7c-dd0b1f992a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install xarray==2024.11.0 zarr==2.18.4 multiformats git+https://github.com/dClimate/py-hamt.git git+https://github.com/dClimate/etl-scripts.git@81549940013b69b57e6b82db339101ac85965ed9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82928721-6668-4e70-801c-575224f8973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "import requests\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from py_hamt import HAMT, IPFSStore, create_zarr_encryption_transformers\n",
    "from multiformats import CID\n",
    "import numcodecs  # For compression if needed\n",
    "\n",
    "# Enecryption Key should be random\n",
    "encryption_key = bytes(32)\n",
    "\n",
    "def prefetch_conus_data(\n",
    "    base_url: str = \"https://psl.noaa.gov/thredds/fileServer/Datasets/cpc_us_precip/RT/precip.V1.0.\",\n",
    "    start_year: int = 2007,\n",
    "    end_year: int = None,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generate a list of netCDF download URLs for CPC CONUS daily precipitation from start_year to end_year.\n",
    "\n",
    "    :param base_url: Prefix of the data location.\n",
    "    :param start_year: Earliest year of data to fetch.\n",
    "    :param end_year: Latest year of data to fetch; defaults to current year if None.\n",
    "    :return: A list of full download URLs, one per year.\n",
    "    \"\"\"\n",
    "    if end_year is None:\n",
    "        end_year = datetime.utcnow().year\n",
    "\n",
    "    download_links = []\n",
    "    for yr in range(start_year, end_year + 1):\n",
    "        download_url = f\"{base_url}{yr}.nc\"\n",
    "        download_links.append(download_url)\n",
    "\n",
    "    return download_links\n",
    "\n",
    "\n",
    "def fetch_data(\n",
    "    download_links: list[str],\n",
    "    local_dir: str = \"cpc_conus_netcdf\",\n",
    "    overwrite: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download .nc files for each link in download_links.  \n",
    "    Uses `requests` (though `wget` via `subprocess` is another approach).\n",
    "\n",
    "    :param download_links: List of full .nc file URLs.\n",
    "    :param local_dir: Local directory in which to save .nc files.\n",
    "    :param overwrite: If False, skip download if file exists.\n",
    "    \"\"\"\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    for url in download_links:\n",
    "        filename = url.split(\"/\")[-1]  # e.g. \"2007.nc\"\n",
    "        local_path = os.path.join(local_dir, filename)\n",
    "\n",
    "        if not overwrite and os.path.exists(local_path):\n",
    "            print(f\"File {local_path} exists; skipping download.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Downloading {url}\")\n",
    "        resp = requests.get(url, stream=True)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        print(f\"Saved to {local_path}\")\n",
    "\n",
    "\n",
    "def transform_to_zarr(\n",
    "    local_dir: str,\n",
    "    output_zarr: str,\n",
    "    varname: str = None,\n",
    "    start_date: str = None,\n",
    "    end_date: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine netCDF files from local_dir into a single xarray Dataset, optionally subsetting by date,\n",
    "    fix missing/fill values, and save to Zarr.\n",
    "\n",
    "    :param local_dir: Directory containing .nc files, each named like YEAR.nc\n",
    "    :param output_zarr: Path to the final Zarr store (e.g. \"cpc_conus.zarr\").\n",
    "    :param varname: Optional name of the variable to keep. If None, keep all.\n",
    "    :param start_date: Optional start date (YYYY-MM-DD). If provided, we do xarrayâ€™s .sel(time=slice(...)).\n",
    "    :param end_date: Optional end date (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    nc_files = list(pathlib.Path(local_dir).glob(\"*.nc\"))\n",
    "    if not nc_files:\n",
    "        raise RuntimeError(f\"No netCDF files found in {local_dir}\")\n",
    "\n",
    "    print(f\"Reading {len(nc_files)} .nc files from {local_dir}\")\n",
    "\n",
    "    # Within a Jupyter Notebook Context setting parallel=False is to prevent read errors. Parallel reads via Dask can lead to file caching hiccups.\n",
    "    ds = xr.open_mfdataset([str(f) for f in nc_files], combine=\"by_coords\", parallel=False)\n",
    "\n",
    "    # If a variable name is provided, keep only that variable.\n",
    "    if varname is not None and varname in ds.data_vars:\n",
    "        ds = ds[[varname]]  # subset to that single variable\n",
    "\n",
    "    # Subset by time if start_date/end_date are provided\n",
    "    if start_date is not None or end_date is not None:\n",
    "        time_slice = slice(start_date, end_date)\n",
    "        ds = ds.sel(time=time_slice)\n",
    "\n",
    "    # Fix fill values or missing values if they exist\n",
    "    # ds = _fix_fill_values(ds)\n",
    "    ds = standardize_and_fix_fill_values(ds)\n",
    "\n",
    "    # Optionally, you can specify chunk sizes or compression\n",
    "    # below is a trivial example of chunking by time. Adjust as needed:\n",
    "    ds = ds.chunk({\"time\": 30})\n",
    "\n",
    "    # For demonstration, apply Blosc compression\n",
    "    for var in ds.data_vars:\n",
    "        ds[var].encoding[\"compressor\"] = numcodecs.Blosc()\n",
    "\n",
    "    print(f\"Writing dataset to Zarr => {output_zarr}\")\n",
    "    ds.to_zarr(output_zarr, mode=\"w\", consolidated=True)\n",
    "    print(\"Zarr creation complete.\")\n",
    "\n",
    "\n",
    "def load_zarr_to_ipfs(zarr_path: str, cid_out_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Put the Zarr store onto IPFS using py-hamt. Returns the root CID as a string.\n",
    "    If cid_out_path is provided, also write the CID to a file.\n",
    "\n",
    "    :param zarr_path: Path to the local Zarr directory.\n",
    "    :param cid_out_path: Optional path to write the CID (e.g. \"cpc_conus.cid\").\n",
    "    :return: The root CID string\n",
    "    \"\"\"\n",
    "    print(f\"Loading {zarr_path} onto IPFS via py-hamt...\")\n",
    "    \n",
    "    # Encrypt only precip\n",
    "    # You can exclude data from being encrypted with exclude_vars=[\"precip\"] for example\n",
    "    encrypt, decrypt = create_zarr_encryption_transformers(\n",
    "        encryption_key, header=\"sample-header\".encode() \n",
    "    )\n",
    "    \n",
    "    hamt_store = HAMT(store=IPFSStore(), transformer_encode=encrypt, transformer_decode=decrypt)  # By default uses local IPFS daemon at 127.0.0.1:5001\n",
    "    \n",
    "    ds = xr.open_zarr(zarr_path)\n",
    "    ds.to_zarr(store=hamt_store, mode=\"w\")\n",
    "\n",
    "    root_cid_str = str(hamt_store.root_node_id)\n",
    "    print(f\"Successfully wrote encrypted data to IPFS. Root CID = {root_cid_str}\")\n",
    "\n",
    "    if cid_out_path:\n",
    "        with open(cid_out_path, \"w\") as f:\n",
    "            f.write(root_cid_str + \"\\n\")\n",
    "\n",
    "    return root_cid_str\n",
    "\n",
    "def standardize_and_fix_fill_values(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Renames lat/lon, sorts coordinates, chunks dataset, fixes fill values, and applies compression.\n",
    "    \"\"\"\n",
    "    # 1. Rename lat/lon to standard naming\n",
    "    ds = ds.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "    \n",
    "    # 2. Sort coordinates to ascending\n",
    "    ds = ds.sortby(\"latitude\", ascending=True)\n",
    "    ds = ds.sortby(\"longitude\", ascending=True)\n",
    "    \n",
    "    # 3. Chunk the dataset (choose chunk sizes appropriate to your use case)\n",
    "    ds = ds.chunk({\"time\": 1769, \"latitude\": 24, \"longitude\": 24})\n",
    "    \n",
    "    for var in ds.data_vars:\n",
    "            da = ds[var]\n",
    "\n",
    "            # Apply compression\n",
    "            # clevel=9 means highest compression level (0-9 scale), we are optimizing for read speed\n",
    "            da.encoding[\"compressor\"] = numcodecs.Blosc(clevel=9)\n",
    "\n",
    "            # Prefer Fill Value over missing_value\n",
    "            da.encoding[\"_FillValue\"] = np.nan\n",
    "            if \"missing_value\" in da.attrs:\n",
    "                del da.attrs[\"missing_value\"]\n",
    "            if \"missing_value\" in da.encoding:\n",
    "                del da.encoding[\"missing_value\"]\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0642f6cd-4eec-4283-86e7-e63b6fd2ed76",
   "metadata": {},
   "source": [
    "After defining the various ETL steps above, it is simply a matter of calling each step in a sequential fashion. In the logs you should see the files being fetched for the dates provided. \n",
    "\n",
    "As you can see, 2007 is given as a start year and 2008 as an end year which is inclusive, and therefore will download the NetCDF files for both years(2007 and 2008). The data is then fetched, however, if the data already already exists locally it will not be downloaded again if the `overwrite` flag is set to false. The next step is to transform the netcdf files to zarr where the `end_date` only includes the first day of 2008 for example sake. Additionally if a `varname` is provided only that variable will be kept on the zarr. \n",
    "\n",
    "Lastly `load_zarr_to_ipfs` will return a CID (read the [Getting Started](./Getting%20Started.ipynb) to learn more) and write it out once the pipeline has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1529dc4-3f2d-4c17-9678-f8f3dc3560e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the modular ETL pipeline\n",
    "\n",
    "# 1. Generate the list of CPC CONUS precipitation .nc URLs (only for years 2007 and 2008, end year is inclusive)\n",
    "download_links = prefetch_conus_data(\n",
    "    start_year=2007,\n",
    "    end_year=2008,  # If you want multiple years, pass a larger end_year\n",
    ")\n",
    "\n",
    "# 2. Download the actual .nc files\n",
    "fetch_data(\n",
    "    download_links=download_links,\n",
    "    local_dir=\"downloaded_data/gis/cpc_conus_netcdf_demo\",\n",
    "    overwrite=False,  # set True if you want to re-download\n",
    ")\n",
    "\n",
    "# 3. Transform the .nc files to Zarr, but only keep the first year \n",
    "transform_to_zarr(\n",
    "    local_dir=\"downloaded_data/gis/cpc_conus_netcdf_demo\",\n",
    "    output_zarr=\"output_data/gis/cpc_conus_demo.zarr\",\n",
    "    varname=\"precip\",         # If you don't know the exact var name, you can omit varname\n",
    "    start_date=\"2007-01-01\",  # Only keep data from Jan 1\n",
    "    end_date=\"2008-01-01\",    # through Jan 1 of 2008 (only includes day one from the second file)\n",
    ")\n",
    "\n",
    "# 4. Put the new \"cpc_conus_demo.zarr\" on IPFS\n",
    "root_cid = load_zarr_to_ipfs(\n",
    "    zarr_path=\"output_data/gis/cpc_conus_demo.zarr\",\n",
    "    cid_out_path=\"output_data/gis/cpc_conus_demo.cid\"\n",
    ")\n",
    "\n",
    "print(f\"Pipeline complete! The root CID is {root_cid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb87f6-8379-4686-8de3-6dbbdecdbe50",
   "metadata": {},
   "source": [
    "# Opening and Reading the Dataset\n",
    "\n",
    "As you can see below, in a similar fashion to the Getting Started Notebook, we go ahead and provide the root hash(CID) of the newly ingested dataset into the HAMT store which is then loaded by xarray. We will show that attempting to read the data without the decrypt or proper decryption key will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54903789-3b4e-4b03-8d1f-0fad02b94380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from py_hamt import HAMT, IPFSStore\n",
    "from multiformats import CID\n",
    "\n",
    "encryption_key = bytes(32)\n",
    "\n",
    "wrong_encryption_key = bytes([1] * 32)\n",
    "\n",
    "decoded_root_cid = CID.decode(root_cid)\n",
    "print(\"Decoded CID from above:\", decoded_root_cid)\n",
    "\n",
    "# Create HAMT instance using the IPFSStore connecting to your locally running IPFS Gateway from your local running IPFS Node\n",
    "hamt = HAMT(store=IPFSStore(gateway_uri_stem=\"http://0.0.0.0:8080\"), root_node_id=decoded_root_cid)\n",
    "\n",
    "ds = xr.open_zarr(store=hamt)\n",
    "print(ds)\n",
    "\n",
    "try:\n",
    "    print(ds.precip.values)\n",
    "except:\n",
    "    print(\"Could not read encrypted values as expected\")\n",
    "\n",
    "# Use bad encryption key\n",
    "encrypt, decrypt = create_zarr_encryption_transformers(\n",
    "    wrong_encryption_key, header=\"sample-header\".encode() \n",
    ")\n",
    "\n",
    "hamt = HAMT(store=IPFSStore(gateway_uri_stem=\"http://0.0.0.0:8080\"), root_node_id=decoded_root_cid, transformer_encode=encrypt, transformer_decode=decrypt)\n",
    "ds = xr.open_zarr(store=hamt)\n",
    "try:\n",
    "    print(ds.precip.values)\n",
    "except:\n",
    "    print(\"Could not read encrypted values as expected due to wrong key\")\n",
    "\n",
    "\n",
    "# Generate the transformers again with the same encryption_key used prior\n",
    "encrypt, decrypt = create_zarr_encryption_transformers(\n",
    "    encryption_key, header=\"sample-header\".encode() \n",
    ")\n",
    "\n",
    "hamt = HAMT(store=IPFSStore(gateway_uri_stem=\"http://0.0.0.0:8080\"), root_node_id=decoded_root_cid, transformer_encode=encrypt, transformer_decode=decrypt)\n",
    "ds = xr.open_zarr(store=hamt)\n",
    "\n",
    "decoded_root_cid = CID.decode(root_cid)\n",
    "print(\"Decoded CID from above:\", decoded_root_cid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ed636-7962-4bf7-8497-7c3e15238829",
   "metadata": {},
   "source": [
    "# Plotting the Data\n",
    "\n",
    "To illustrate the data we go ahead and plot some of the precipitation information for New York using encrypted data :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec4e87-b17c-4a41-99ce-4a63c14f6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lon_range = (360 - 74.25, 360 - 73.7)  # Converted to 285.75 - 286.3\n",
    "lat_range = (40.5, 45.0)  # Latitude remains the same\n",
    "\n",
    "ny_precip = ds.sel(latitude=slice(*lat_range), longitude=slice(*lon_range))\n",
    "print(ny_precip)\n",
    "\n",
    "if ny_precip.precip.count() > 0:\n",
    "    print(\"Precipitation data exists for this range.\")\n",
    "else:\n",
    "    print(\"No precipitation data for the specified range.\")\n",
    "\n",
    "# Compute the average precipitation over the region\n",
    "ny_precip_mean = ny_precip.precip.mean(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "ny_precip_mean.plot(marker='o', color='blue')\n",
    "plt.title(\"Precipitation in New York (Daily Mean)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Precipitation (mm)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18806670-7dbb-47a4-b84e-f2d091e8d06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
